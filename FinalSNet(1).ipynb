{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalSNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyuvq5X0s4wv",
        "colab_type": "text"
      },
      "source": [
        "#SEMANTIC SEGMENTATION to find Silhouttes \n",
        "#PASCAL VOC DATASET ON KAGGLE\n",
        "[Kaggle](https://www.kaggle.com/huanghanchina/pascal-voc-2012)\n",
        "(https://www.kaggle.com/huanghanchina/pascal-voc-2012\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsxXAuW0u9Os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Please upload your kaggle.json file before using it\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d huanghanchina/pascal-voc-2012\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/pascal-voc-2012.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iGKrNsdhSsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDwU4rnawx4R",
        "colab_type": "text"
      },
      "source": [
        "#GOOGLE COLAB FILE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjmk9KaqcGSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Connecting google drive  with the notebook \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "msb6iyzXwvoS",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Reading the trainval file which contains the names of images that are in our dataset\n",
        "f_name = pd.read_csv(\"/content/VOC2012/ImageSets/Segmentation/trainval.txt\", sep=\"\\n\",names=[\"a\"])\n",
        "#np.random.shuffle(df.values)\n",
        "#df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLyflk9nwvou",
        "colab": {}
      },
      "source": [
        "###########TO convert the images############\n",
        "#this is to get the Black and white image, as .convert('1') in Imgae.open() dint help.//I find a bug as when I use .convert it looses my color Pallette of the images\n",
        "#for more infomation on Pallette please read about .png palette.\n",
        "#This will take some time to read the images(Slow Process)\n",
        "\n",
        "\n",
        "#Background is 0(Black), So i marked it with 1(white) and all other colors as black(0)\n",
        "def cvtImag(ak1):\n",
        "  ak2=ak1\n",
        "  for i in range(ak1.shape[1]):\n",
        "    for j in range(ak1.shape[2]):\n",
        "      if ak1[0][i][j]!=0:\n",
        "        ak2[0][i][j]=0\n",
        "      else:\n",
        "        ak2[0][i][j]=1\n",
        "  return ak2  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjKqcS3-h1GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRxxD2Kou3Ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_img.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rzR35VNuxCzI",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms \n",
        "\n",
        "#Center Crop leads to image size changes to 128*128\n",
        "\n",
        "transform=transforms.Compose([\n",
        " # transforms.resize\n",
        "  transforms.CenterCrop((128,128)),\n",
        "  transforms.ToTensor(),\n",
        " ])\n",
        "#to_pil(images[111])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_hOSUBt8054",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K-cGEXK2JPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing purpose(Ignore it)\n",
        "import numpy as np\n",
        "#data1=np.load(\"/content/drive/My Drive/Data/train_data(Segmentation).npy\")\n",
        "#data1=np.save(\"/content/drive/My Drive/Data/train_data(Segmentation).npy\",train_imag)\n",
        "#np.save(\"/content/drive/My Drive/Data/train_data1(Segmentation).npy\",train_imag1)\n",
        "#np.save(\"/content/drive/My Drive/Data/train_data2(Segmentation).npy\",train_imag2)\n",
        "\n",
        "\"\"\"\n",
        "np.save(\"/content/drive/My Drive/Data/label_data(Segmentation).npy\",images)\n",
        "np.save(\"/content/drive/My Drive/Data/label_data1(Segmentation).npy\",images1)\n",
        "np.save(\"/content/drive/My Drive/Data/label_data2(Segmentation).npy\",images2)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afpADhrX_DTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#(testing purpose)\n",
        "train_imag=np.save(\"/content/drive/My Drive/Data/train_data(Segmentation).npy\")\n",
        "train_imag1=np.save(\"/content/drive/My Drive/Data/train_data1(Segmentation).npy\")#,train_imag1)\n",
        "train_imagn2=p.save(\"/content/drive/My Drive/Data/train_data2(Segmentation).npy\")#,train_imag2)\n",
        "\n",
        "\"\"\"\n",
        "np.save(\"/content/drive/My Drive/Data/label_data(Segmentation).npy\",images)\n",
        "np.save(\"/content/drive/My Drive/Data/label_data1(Segmentation).npy\",images1)\n",
        "np.save(\"/content/drive/My Drive/Data/label_data2(Segmentation).npy\",images2)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K0eNp_0ixCzg",
        "colab": {}
      },
      "source": [
        "import PIL.Image as Image\n",
        "import os\n",
        "import torch\n",
        "#pl=os.listdir(\"/content/VOC2012/SegmentationObject\")\n",
        "####Time for reading the image################\n",
        "\n",
        "#f_name is list contations the names of images  \n",
        "images=torch.zeros(len(f_name),1,128,128)#It makes a tensor of zeros in which we will store the images\n",
        "\n",
        "#images1=torch.zeros(len(df),1,128,128)\n",
        "\n",
        "#images2=torch.zeros(len(df),1,128,128)\n",
        "\n",
        "for i in range(len(df)):\n",
        "    ak=Image.open((\"/content/VOC2012/SegmentationObject\")+\"/\"+df['a'][i]+\".png\")#.convert('1')\n",
        "    ak=ak.resize((150,140))#Resize the images to 150*140\n",
        "    \n",
        "    ak1=transform(ak)#transform---Crop and Totensor \n",
        "    ak1=cvtImag(ak1)#Convert mask_images to silhoutte \n",
        "    images[i]=ak1#Storing in the array \n",
        "    \n",
        "    \"\"\"ak1=transforms1(ak)\n",
        "    ak1=cvtImag(ak1)\n",
        "    images1[i]=ak1\n",
        "    \n",
        "    ak1=transforms2(ak)\n",
        "    ak1=cvtImag(ak1)\n",
        "    images2[i]=ak1\n",
        "    \"\"\"\n",
        "    #print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r96TSJvIFMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For Data Augmentation(Testing Purpose)  \n",
        "  import PIL\n",
        "transforms1=transforms.Compose([\n",
        "    transforms.CenterCrop((128,128)),\n",
        "    transforms.ColorJitter(hue=0.5,saturation=0.5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(40,resample=PIL.Image.BILINEAR),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "#Data Augmentation(Testing Purpose)\n",
        "\n",
        "transforms2=transforms.Compose([\n",
        "    transforms.CenterCrop((128,128)),\n",
        "    transforms.ColorJitter(hue=0.3,saturation=0.8),\n",
        "    transforms.RandomVerticalFlip(p=0.6),\n",
        "    transforms.RandomAffine(40),#,translate=(.5,.7),scale=(10,20),shear=(3,7),resample=PIL.Image.BICUBIC,fillcolor=(112)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8hWE0Fc60kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTBHqS0Sr1UJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4VGnytmBwvo9",
        "colab": {}
      },
      "source": [
        "\n",
        "#pl=os.listdir(\"/content/VOC2012/SegmentationObject\")\n",
        "\n",
        "\n",
        "train_imag=torch.zeros(len(f_name),3,128,128)\n",
        "#train_imag1=torch.zeros(len(f_name),3,128,128)\n",
        "#train_imag2=torch.zeros(len(f_name),3,128,128)\n",
        "\n",
        "#################################################This will read .jpg file means our input to the NN images(Images whose silllout to find).################################ \n",
        "\n",
        "for i in range(len(f_name)):\n",
        "    ak=Image.open((\"/content/VOC2012/JPEGImages\")+\"/\"+df['a'][i]+\".jpg\")()\n",
        "    #ak=ak.resize((150,140))\n",
        "    ak1=transform(ak)\n",
        "    train_imag[i]=ak1\n",
        "    \n",
        "    \"\"\"ak1=transforms1(ak)\n",
        "    train_imag1[i]=ak1\n",
        "    \n",
        "    ak1=transforms2(ak)\n",
        "    train_imag2[i]=ak1\n",
        "    \"\"\"\n",
        "    #print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mW2pzF_Xw58H",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd5FDpE2witR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########BATCHES OF 10######### \n",
        "\n",
        "#Training Image############\n",
        "#In Batch of 10\n",
        "train_imag=train_imag[:2840]\n",
        "img=train_imag.view(142,20,3,128,128)\n",
        "\"\"\"\n",
        "target=images[:2840]\n",
        "targ=target.view(142,20,1*128,128)\n",
        "\n",
        "train_imag1=train_imag1[:2840]\n",
        "img1=train_imag1.view(142,20,3,128,128)\n",
        "target1=images1[:2840]\n",
        "targ1=target1.view(142,20,1*128,128)\n",
        "\n",
        "train_imag2=train_imag2[:2840]\n",
        "img2=train_imag2.view(142,20,3,128,128)\n",
        "target2=images2[:2840]\n",
        "targ2=target2.view(142,20,1*128,128)\n",
        "\n",
        "\n",
        "##########1000 only#######\n",
        "train_img=img[:100]\n",
        "train_target=targ[:100]\n",
        "valid_img=img[100:122]\n",
        "valid_target=targ[100:122]\n",
        "\n",
        "train_img1=img1[:100]\n",
        "train_target1=targ1[:100]\n",
        "valid_img1=img1[100:122]\n",
        "valid_target1=targ1[100:122]\n",
        "\n",
        "train_img2=img2[:100]\n",
        "train_target2=targ2[:100]\n",
        "valid_img2=img2[100:122]\n",
        "valid_target2=targ2[100:122]\n",
        "\n",
        "\n",
        "list_train=[]\n",
        "list_train.append(train_img)\n",
        "list_train.append(train_img1)\n",
        "list_train.append(train_img2)\n",
        "\n",
        "list_tlabel=[]\n",
        "list_tlabel.append(train_target)\n",
        "list_tlabel.append(train_target1)\n",
        "list_tlabel.append(train_target2)\n",
        "\n",
        "\n",
        "list_tValid=[]\n",
        "list_tValid.append(valid_img)\n",
        "list_tValid.append(valid_img1)\n",
        "list_tValid.append(valid_img2)\n",
        "\n",
        "list_ValidL=[]\n",
        "list_ValidL.append(valid_target)\n",
        "list_ValidL.append(valid_target1)\n",
        "list_ValidL.append(valid_target2)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep6F_zW9wlbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Training Image(input)############\n",
        "#In Batch of 1\n",
        "train_imag=train_imag[:100]\n",
        "img=train_imag.view(100,1,3,128,128)\n",
        "\n",
        "# Target Image############\n",
        "#In Batch of 1\n",
        "target=images[:100]\n",
        "targ=target.view(100,1,128,128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkv2KNYk_ugS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI0GPcvwxgMd",
        "colab_type": "text"
      },
      "source": [
        "**Model Starts From Here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNO4bgqHvEhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear,Conv2d,MaxPool2d,LeakyReLU,Conv3d,Tanh,Sigmoid,ConvTranspose2d\n",
        "import torch\n",
        "import torchvision.models as models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RpFPbQeuXBf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#  model Architecture\n",
        "![Screenshot_20190313-002300-4](https://user-images.githubusercontent.com/40520042/64706513-34c2a100-d4cf-11e9-9482-e1f968ba9acc.jpg)\n",
        "![oie_ZWVBh1hmQ4wM](https://user-images.githubusercontent.com/40520042/64707427-9afbf380-d4d0-11e9-8434-a0f3604d2a25.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHRhb5zavGvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l,m,n=0,0,0\n",
        "class GRUNet(nn.Module):\n",
        "  def __init__(self):\n",
        "        print('\\nGruInitializing')\n",
        "        super(GRUNet,self).__init__()\n",
        "        \n",
        "        self.is_x_tensor4 = False\n",
        "        \n",
        "        self.batch_size, self.img_w, self.img_h=10,128,128\n",
        "        \n",
        "        self.input_shape = (self.batch_size, 3, self.img_w, self.img_h)\n",
        "        \n",
        "        #number of filters for each convolution layer in the encoder\n",
        "        self.n_convfilter = [96, 128, 256, 256, 256, 256]  \n",
        "        \n",
        "        #the dimension of the fully connected layer\n",
        "        self.n_fc_filters = [1024]\n",
        "        \n",
        "        #number of filters for each 2d convolution layer in the decoder\n",
        "        self.n_deconvfilter = [256, 256, 256, 128, 96, 2]\n",
        "   \n",
        "        self.encoder=encoder(self.input_shape,self.n_convfilter,\\\n",
        "                            self.n_fc_filters)\n",
        "        \n",
        "        self.decoder=decoder(self.n_deconvfilter,n_class=2)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    output,s1,s2,s3,s4,s5= self.encoder(x)#c1,c2,etc are output of skip connections that further add to decoder(Please see the Architcture model)\n",
        "    final_sil=self.decoder(output,s1,s2,s3,s4,s5)\n",
        "    \n",
        "    return final_sil\n",
        "      \n",
        "  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw01PwZEjPat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGJukwBZia7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrdSqfJ6jhxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_5NsaTpvPsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############Please visit the model Architecture Image for clear understanding#############################\n",
        "\n",
        "class encoder(nn.Module):\n",
        "  def __init__(self,input_shape,n_convfilter,\\\n",
        "               n_fc_filters):\n",
        "        print(\"\\ninitalizing \\\"encoder\\\"\")\n",
        "        super(encoder,self).__init__()\n",
        "        #conv1\n",
        "        self.conv1a = Conv2d(input_shape[1], n_convfilter[0], 7, padding=3,stride=1)#\n",
        "        self.conv1b = Conv2d(n_convfilter[0], n_convfilter[0], 3, padding=1,stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d( n_convfilter[0])\n",
        "        \n",
        "        #conv2\n",
        "        self.conv2a = Conv2d(n_convfilter[0], n_convfilter[1], 3, padding=1,stride=1)\n",
        "        self.conv2b = Conv2d(n_convfilter[1], n_convfilter[1], 3, padding=1,stride=1)\n",
        "        self.conv2c = Conv2d(n_convfilter[0], n_convfilter[1], 1)\n",
        "        self.bn2 = nn.BatchNorm2d( n_convfilter[1])\n",
        "        #conv3\n",
        "        self.conv3a = Conv2d(n_convfilter[1], n_convfilter[2], 3, padding=1,stride=1)\n",
        "        self.conv3b = Conv2d(n_convfilter[2], n_convfilter[2], 3, padding=1,stride=1)\n",
        "        self.conv3c = Conv2d(n_convfilter[1], n_convfilter[2], 1)\n",
        "        self.bn3 = nn.BatchNorm2d( n_convfilter[2])\n",
        "        #conv4\n",
        "        self.conv4a = Conv2d(n_convfilter[2], n_convfilter[3], 3, padding=1,stride=1)\n",
        "        self.conv4b = Conv2d(n_convfilter[3], n_convfilter[3], 3, padding=1,stride=1)\n",
        "        self.bn4 = nn.BatchNorm2d( n_convfilter[3])\n",
        "        #conv5\n",
        "        self.conv5a = Conv2d(n_convfilter[3], n_convfilter[4], 3, padding=1,stride=1)\n",
        "        self.conv5b = Conv2d(n_convfilter[4], n_convfilter[4], 3, padding=1,stride=1)\n",
        "        self.conv5c = Conv2d(n_convfilter[3], n_convfilter[4], 1)\n",
        "        self.bn5 = nn.BatchNorm2d( n_convfilter[4])\n",
        "        #conv6\n",
        "        self.conv6a = Conv2d(n_convfilter[4], n_convfilter[5], 3, padding=1,stride=1)\n",
        "        self.conv6b = Conv2d(n_convfilter[5], n_convfilter[5], 3, padding=1,stride=1)\n",
        "        self.bn67 = nn.BatchNorm2d( n_convfilter[5])\n",
        "        \n",
        "        #conv7\n",
        "        self.conv7a = Conv2d(n_convfilter[5], n_convfilter[5], 3, padding=1,stride=1)\n",
        "        self.conv7b = Conv2d(n_convfilter[5], n_convfilter[5], 3, padding=1,stride=1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        #pooling layer\n",
        "        self.pool1 = MaxPool2d(kernel_size= 2,stride=2)#,return_indices=True)\n",
        "        self.pool2 = MaxPool2d(kernel_size=1,stride=2)#,return_indices=True)\n",
        "        self.gpool=nn.AvgPool2d(kernel_size=2)\n",
        "        \n",
        "        #nonlinearities of the network\n",
        "        self.leaky_relu = LeakyReLU(negative_slope= 0.01)\n",
        "        #self.sigmoid = Sigmoi\n",
        "        self.tanh = Tanh()\n",
        "        \n",
        "        self.conv8a = Conv2d(n_convfilter[5], 1024, 1, padding=0,stride=1)\n",
        "        self.bn8=nn.BatchNorm2d(1024)\n",
        "        self.drop=nn.Dropout2d(0.5)\n",
        "        #self.fc7 = Linear(1*1*256, 1024) \n",
        "            \n",
        "        \n",
        "            \n",
        "  def forward(self, x):\n",
        "        \n",
        "        #x is the input and the size of x is (batch_size, channels, heights, widths).\n",
        "        \n",
        "        conv1a = self.conv1a(x)\n",
        "        rect1a = self.leaky_relu(conv1a)\n",
        "        conv1b = self.conv1b(rect1a)\n",
        "        \n",
        "        rect1 = self.leaky_relu(conv1b)\n",
        "        pool1 = self.pool1(rect1)\n",
        "        #pool1=self.drop(pool1)\n",
        "        pool1=self.bn1(pool1)\n",
        "        \n",
        "        conv2a = self.conv2a(pool1)\n",
        "        rect2a = self.leaky_relu(conv2a)\n",
        "        conv2b = self.conv2b(rect2a)\n",
        "        rect2 = self.leaky_relu(conv2b)\n",
        "        conv2c = self.conv2c(pool1)\n",
        "        res2 = conv2c + rect2\n",
        "        pool2= self.pool2(res2)\n",
        "        \n",
        "        #pool2=self.drop(pool2)\n",
        "        pool2=self.bn2(pool2)\n",
        "        \n",
        "        conv3a = self.conv3a(pool2)\n",
        "        rect3a = self.leaky_relu(conv3a)\n",
        "        conv3b = self.conv3b(rect3a)\n",
        "        rect3 = self.leaky_relu(conv3b)\n",
        "        conv3c = self.conv3c(pool2)\n",
        "        res3 = conv3c + rect3\n",
        "        pool3 = self.pool2(res3)\n",
        "        \n",
        "        #pool3=self.drop(pool3)\n",
        "        pool3=self.bn3(pool3)\n",
        "        \n",
        "        conv4a = self.conv4a(pool3)\n",
        "        rect4a = self.leaky_relu(conv4a)\n",
        "        conv4b = self.conv4b(rect4a)\n",
        "        rect4 = self.leaky_relu(conv4b)\n",
        "        pool4= self.pool2(rect4)\n",
        "        \n",
        "        #pool4=self.drop(pool4)\n",
        "        pool4=self.bn4(pool4)\n",
        "        \n",
        "        conv5a = self.conv5a(pool4)\n",
        "        rect5a = self.leaky_relu(conv5a)\n",
        "        conv5b = self.conv5b(rect5a)\n",
        "        rect5 = self.leaky_relu(conv5b)\n",
        "        conv5c = self.conv5c(pool4)\n",
        "        res5 = conv5c + rect5\n",
        "        pool5 = self.pool2(res5)\n",
        "        \n",
        "        pool5=self.drop(pool5)\n",
        "        pool5=self.bn5(pool5)\n",
        "        \n",
        "        conv6a = self.conv6a(pool5)\n",
        "        rect6a = self.leaky_relu(conv6a)\n",
        "        conv6b = self.conv6b(rect6a)\n",
        "        rect6 = self.leaky_relu(conv6b)\n",
        "        res6 = pool5 + rect6\n",
        "        pool6= self.pool2(res6)\n",
        "        \n",
        "        pool6=self.drop(pool6)\n",
        "        pool6=self.bn67(pool6)\n",
        "        \n",
        "        conv7a = self.conv6a(pool6)\n",
        "        rect7a = self.leaky_relu(conv7a)\n",
        "        conv7b = self.conv6b(rect7a)\n",
        "        rect7 = self.leaky_relu(conv7b)\n",
        "        res7 = pool6 + rect7\n",
        "        pool7 = self.pool2(res7)\n",
        "        \n",
        "        pool8=self.gpool(pool7)\n",
        "                \n",
        "        \n",
        "        #pool9 = pool8.view(pool8.size(0), -1)\n",
        "        #print(pool8.shape)\n",
        "        \n",
        "        fc7 = self.conv8a(pool8)\n",
        "        rect7 = self.leaky_relu(fc7)\n",
        "        rect7=self.bn8(rect7)\n",
        "\n",
        "        \n",
        "        #print(rect7.shape)\n",
        "        \n",
        "        return rect7,pool1,pool2,pool3,pool4,pool5\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t3CTSK5vVGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################OUR NETWORK#########################\n",
        "###################DECODER\n",
        "\n",
        "class decoder(nn.Module):\n",
        "    def __init__(self, n_deconvfilter,n_class):\n",
        "        self.n_class=n_class\n",
        "        print(\"\\ninitializing \\\"decoder\\\"\")\n",
        "        super(decoder, self).__init__()\n",
        "        #2d conv10\n",
        "        self.conv10 = ConvTranspose2d(1024, n_deconvfilter[0], 3,stride=2, output_padding=1)\n",
        "        self.bn10=nn.BatchNorm2d(n_deconvfilter[0])\n",
        "        \n",
        "        \n",
        "        #2d conv11\n",
        "        self.conv11 = ConvTranspose2d(n_deconvfilter[0], n_deconvfilter[1], 3, padding=1,stride=2,output_padding=1)#((4-1)*2+3-2*1)\n",
        "        self.bn11=nn.BatchNorm2d(n_deconvfilter[1])\n",
        "        \n",
        "        #2d conv12\n",
        "        self.conv12 = ConvTranspose2d(n_deconvfilter[1], n_deconvfilter[2], 3, padding=1,stride=2,output_padding=1)\n",
        "        self.bn12=nn.BatchNorm2d(n_deconvfilter[2])\n",
        "        \n",
        "        #2d conv13\n",
        "        self.conv13 = ConvTranspose2d(n_deconvfilter[2], n_deconvfilter[3], 3, padding=1,stride=2,output_padding=1)\n",
        "        self.bn13=nn.BatchNorm2d(n_deconvfilter[3])\n",
        "        \n",
        "        #2d conv14\n",
        "        self.conv14 = ConvTranspose2d(n_deconvfilter[3], n_deconvfilter[4], 4, padding=1,stride=2)\n",
        "        self.bn14=nn.BatchNorm2d(n_deconvfilter[4])\n",
        "        \n",
        "        self.conv15 = ConvTranspose2d(n_deconvfilter[4], 2, 4, padding=1,stride=2)\n",
        "        self.bn15=nn.BatchNorm2d(2)\n",
        "        \n",
        "        self.leaky_relu = LeakyReLU(negative_slope= 0.01)\n",
        "        self.softmax=nn.Softmax2d()\n",
        "        \n",
        "    def forward(self, rect7,c1,c2,c3,c4,c5):\n",
        "        \n",
        "        conv10 = (self.conv10(rect7))\n",
        "        rect10 = self.leaky_relu(conv10)\n",
        "        \n",
        "        rect10=rect10+c5\n",
        "        rect10=self.bn10(rect10)\n",
        "        \n",
        "        conv11 = (self.conv11(rect10))\n",
        "        rect11 = self.leaky_relu(conv11)\n",
        "        rect11=rect11+c4 \n",
        "        rect11=self.bn11(rect11)\n",
        "      \n",
        "        conv12 = (self.conv12(rect11))\n",
        "        rect12 = self.leaky_relu(conv12)\n",
        "        rect12=rect12+c3\n",
        "        rect12=self.bn12(rect12)\n",
        "        \n",
        "        \n",
        "        conv13 =(self.conv13(rect12))\n",
        "        rect13 = self.leaky_relu(conv13)\n",
        "        rect13=rect13+c2\n",
        "        rect13=self.bn13(rect13)\n",
        "        \n",
        "        conv14 =( self.conv14(rect13))\n",
        "        rect14 = self.leaky_relu(conv14)\n",
        "        rect14=rect14+c1\n",
        "        rect14=self.bn14(rect14)\n",
        "        \n",
        "        conv15=(self.conv15(rect14))\n",
        "        \n",
        "        h=conv15\n",
        "        h=self.softmax(conv15)\n",
        "       \n",
        "        return h\n",
        "      \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZLrc3yILxWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########INITLASISATON WEIGHTS##################################\n",
        "#weights----XAVIER Init\n",
        "#bias------0.1\n",
        "def init_weights(m):\n",
        "  #if isinstance(m,nn.Conv2d):\n",
        "    torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "    #print(\"yes\")\n",
        "    m.bias.data.fill_(0.1)\n",
        "    #m.weight.data.fill_(0.1)\n",
        "    #torch.nn.init.xavier_uniform(m.bias.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92hA2VY9_yh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIpWgPjniCVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######Make a list of All the conv layers to initalise by xavier and bias \n",
        "ob=[]\n",
        "ob.append(model.encoder.conv1a)\n",
        "ob.append(model.encoder.conv1b)\n",
        "ob.append(model.encoder.conv2a)\n",
        "ob.append(model.encoder.conv2b)\n",
        "ob.append(model.encoder.conv2c)\n",
        "ob.append(model.encoder.conv3a)\n",
        "ob.append(model.encoder.conv3b)\n",
        "ob.append(model.encoder.conv3c)\n",
        "ob.append(model.encoder.conv4a)\n",
        "ob.append(model.encoder.conv4b)\n",
        "ob.append(model.encoder.conv5a)\n",
        "ob.append(model.encoder.conv5b)\n",
        "ob.append(model.encoder.conv5c)\n",
        "ob.append(model.encoder.conv6a)\n",
        "ob.append(model.encoder.conv6b)\n",
        "ob.append(model.encoder.conv7a)\n",
        "ob.append(model.encoder.conv7b)\n",
        "ob.append(model.encoder.conv8a)\n",
        "\n",
        "ob.append(model.decoder.conv10)\n",
        "ob.append(model.decoder.conv11)\n",
        "ob.append(model.decoder.conv12)\n",
        "ob.append(model.decoder.conv13)\n",
        "ob.append(model.decoder.conv14)\n",
        "ob.append(model.decoder.conv15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uFs5PDbxjHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=GRUNet().cuda()###creating model and move to GPU\n",
        "for i in range(len(ob)):###initialising weights and biases\n",
        "  init_weights(ob[i])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjLcwS2wlyUk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EFtuTtZzOUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####loading checkpoint\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/model_checkpoint(SNet)_Best.pth\"))\n",
        "model.eval()\n",
        "#model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33PuuYfukG6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y942QUrCkSMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Testing########### \n",
        "test=train_imag[1].view(1,3,128,128)#####convert to size 1*3*128*128(batch 1)\n",
        "pm=model(test.cuda())\n",
        "\n",
        "am=torch.argmax(pm,dim=1)##our model output 1*128*128*2 WHERE 2 represent 2 layers\n",
        "                         ##1st layer for backgroundy\n",
        "                         ##2nd layer for object \n",
        "                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec459DDplqka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSjpOMiIwpvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1n0SZrCH4c9",
        "colab_type": "text"
      },
      "source": [
        "# **Training the model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F_6nLz7vXgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "#Cross entropy as loss function\n",
        "criterion =  nn.CrossEntropyLoss()\n",
        "\n",
        "#Adam optimizer with learning rate of 0.1 in the starting\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.1)#,eps=0.001)#,betas=(0.9,0.999))\n",
        "\n",
        "#Scheduling the learning rate to be decreased via cosine \n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(train_img), eta_min=0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCuhtDsW24Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOXkV5Nhvm5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 20\n",
        "\n",
        "\n",
        "#valid_loss_min = np.Inf # track change in validation loss\n",
        "#model.train()\n",
        "train_loss = 0.0\n",
        "#valid_loss=0.0\n",
        "import datetime\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  \n",
        "    \n",
        "    print(\"EPOCH\" ,epoch)\n",
        "    print(\"Learning Rate\",scheduler.get_lr())\n",
        "    train_loss = 0.0 \n",
        "    #valid_loss=0.0\n",
        "    #model.train()\n",
        "    for j in range(len(list_train)):\n",
        "          train_img5=list_train[j]\n",
        "          train_target5=list_tlabel[j]         \n",
        "          for i in range(len(train_img)):\n",
        "\n",
        "                data=train_img5[i].cuda()\n",
        "                tar=train_target5[i].cuda()\n",
        "                tar=tar.long()\n",
        "\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                output=model(data)\n",
        "\n",
        "                loss = criterion(output, tar)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()*data.size(0)\n",
        "\n",
        "                \"\"\"if i==43 and j==2:\n",
        "                  print(\"loss at 43 \",train_loss)\n",
        "                 \"\"\"\n",
        "    scheduler.step() #Modifying the Learning rate            \n",
        "#FOR NOW I commented validation code.\n",
        "                  \"\"\"\n",
        "    model.eval() \n",
        "    for j in range(len((list_tValid))):\n",
        "          valid_img5=list_tValid[j]\n",
        "          valid_target5=list_ValidL[j]         \n",
        "    \n",
        "          for i in range(len(valid_img5)):\n",
        "                data=valid_img5[i].cuda()\n",
        "                tar=valid_target5[i].cuda().long()\n",
        "                loss = torch.mean(criterion(model(data), tar))\n",
        "                valid_loss += loss.item()*data.size(0)\n",
        "\"\"\"\n",
        "      \n",
        "    print(\"ALL ABOUT LOSS(training)--------\",(train_loss/(len(images)*4)))\n",
        "    #print(\"ALL ABOUT LOSS(valid)--------\",(valid_loss/(len(valid_target)*4)))\n",
        "    print(\" \")\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhffQaafiGyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFScPMebfV3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########Shifting the check_point to google drive \n",
        "import shutil\n",
        "torch.save(model.state_dict(),\"SNet_checkpoint(50)Iter.pth\")\n",
        "shutil.copy(\"/content/SNet_checkpoint(50)Iter.pth\",\"/content/drive/My Drive/Data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF5XDfQxvwJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuqHmh2uv3xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B34AgM4Zv4n8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk2UoJZRLj42",
        "colab_type": "text"
      },
      "source": [
        "# Code below Testing Purpose(Please ignore it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlsxzz1evy03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pm=model(test.cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8exR9PIivzlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUT05vcLfgL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ak1.shape\n",
        "pl=ak1.view(1,3,128,128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm0RLR4uF9bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ak1=Image.open(\"/content/image-1465348_960_720.jpg\")\n",
        "ak=transform(ak1)\n",
        "ak=ak.view(1,3,128,128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQqNqjE1uoYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sueZG_yfg0B",
        "colab_type": "text"
      },
      "source": [
        "24,3,1,0\n",
        "\n",
        "Problem\n",
        "9,10(with Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmtWeD3qy7I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_pil(ak)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2P1SIpvn4K7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GwrUt_sDz4_",
        "colab_type": "text"
      },
      "source": [
        "#8,8#5,3#0,3\n",
        "Fails-0,0,#0,1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOL9CcLBnoha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=train_imag[25].view(1,3,128,128)\n",
        "pm=model(test.cuda())\n",
        "\n",
        "am=torch.argmax(pm,dim=1)\n",
        "\n",
        "to_pil(am.float().cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWs8j8zxwCVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "to_pil = torchvision.transforms.ToPILImage()#.convert('L')\n",
        "to_pil(train_imag[74])\n",
        "\n",
        "\n",
        "#to_pil(tt)\n",
        "#to_pil(am.float().cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy6r3KHYy_2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=train_imag[5].view(1,3,128,128)\n",
        "pm=model(test.cuda())\n",
        "\n",
        "am=torch.argmax(pm,dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAl-3KKyqOFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "to_pil = torchvision.transforms.ToPILImage()#.convert('L')\n",
        "to_pil(am.cpu())\n",
        "\n",
        "\n",
        "#to_pil(tt)\n",
        "#to_pil(am.float().cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkKdejcTxz3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "007fajnHqP9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c=0\n",
        "for i in range(128):\n",
        "  for j in range(128):\n",
        "    if a[i][j]==1:\n",
        "      c+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT8VqT5DYJ2I",
        "colab_type": "text"
      },
      "source": [
        "VIEWPOINT ESTIMATION"
      ]
    }
  ]
}